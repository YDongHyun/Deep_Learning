{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Tensorflow_t1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/M95L4FEjSWxemHc5B5Tg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YDongHyun/Deep_Learning/blob/main/Tensorflow_t1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Flow 시작하기\n",
        "\n",
        "##텐서플로우 공부 -1  22.02*.08*\n",
        "\n"
      ],
      "metadata": {
        "id": "cJ4uMBFsRvzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. 프로그램에 텐서플로 라이브러리를 임포트"
      ],
      "metadata": {
        "id": "is8-w2yQR4UG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfkiLxDJRtCI",
        "outputId": "73b95e22-ad96-46e3-d62c-4aca65a88d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 489.5 MB 33 kB/s \n",
            "\u001b[K     |████████████████████████████████| 463 kB 45.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 35.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-gpu==2.7.0-rc1\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist  #Mnist 데이터셋 로드  (손으로 쓴 숫자들로 이루어진 대형 데이터베이스)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # 샘플 값을 정수에서 부동소수로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYR_tyP9SDwy",
        "outputId": "f6e62c1e-3b5d-4482-f105-590008b6d96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  층을 차례대로 쌓아 tf.keras.Sequential 모델을 생성\n",
        "\n",
        "\n",
        "\n",
        "1. 옵티마이저는 학습 데이터(Train data)셋을 이용하여 모델을 학습 할 때 데이터의 실제 결과와 모델이 예측한 결과를 기반으로 잘 줄일 수 있게 만들어주는 역할\n",
        "\n",
        "2.  Dense의 주요 인자\n",
        "    - 첫번째 인자(units): 출력 뉴런의 수를 설정\n",
        "    - input_dim : 입력 뉴련의 수를 설정\n",
        "    - kernel_initilizer : 가중치를 초기화하는 방법을 설정\n",
        "    - uniform : 균일 분포\n",
        "    - normal : 가우시안 분포\n",
        "    - activation : 활성화함수를 설정\n",
        "    - linear : 디폴트 값으로 입력값과 가중치로 계산된 결과 값이 그대로 출력\n",
        "    - sigmoid : 시그모이드 함수로 이진분류에서 출력층에 주로 쓰임\n",
        "    - softmax : 소프드맥스 함수로 다중클래스 분류문제에서 출력층에 주로 쓰임\n",
        "    - relu: Rectified Linear Unit 함수로 은닉층에서 주로 쓰임\n",
        "\n",
        "3. 옵티마이저/ 손실함수\n",
        " \n",
        " \t\n",
        "   \n",
        "    1-1 손실 함수 (Loss Function)\n",
        "    손실 함수는 실제값과 예측값의 차이(loss, cost)를 수치화해주는 함수이다.\n",
        "\n",
        "    오차가 클수록 손실 함수의 값이 크고, 오차가 작을수록 손실 함수의 값이 작아진다.\n",
        "\n",
        "    손실 함수의 값을 최소화 하는 W, b를 찾아가는것이 학습 목표이다.\n",
        "\n",
        "    회귀 : 평균제곱오차 / 분류 : 크로스 엔트로피\n",
        "\n",
        "\n",
        "    1-1-1 평균 제곱 오차 (Mean Squared Error, MSE)\n",
        "\n",
        "    연속형 변수를 예측할 때 사용한다\n",
        "    \n",
        "\n",
        "\n",
        "    1-1-2 크로스 엔트로피 (Cross-Entropy)\n",
        "\n",
        "    낮은 확률로 예측해서 맞추거나, 높은 확률로 예측해서 틀리는 경우 loss가 더 크다.\n",
        "\n",
        "    이진분류 : binary_crossentropy / 다중분류 : categorical_crossentropy\n",
        "\n",
        "    y : 실제값(0혹은1) / y^ : 예측값(확률)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    1-2. 최적화 (Optimizer, 옵티마이저)\n",
        "    손실함수를 줄여나가면서 학습하는 방법은 어떤 optimizer를 사용하느냐에 따라 달라진다.\n",
        " \n",
        "    1-2-1 경사 하강법(Gradient Descent)\n",
        "\n",
        "    가장 기본적인 optimizer 알고리즘\n",
        "    경사를 따라 내려가면서 가중치(w) 업데이트 시킨다.\n",
        "    손실함수를 최소화하기 위하여 반복적으로 파라미터를 조정해나가는 방법\n",
        "    학습률(learning rate)이 너무 크면 학습시간이 짧아지나 전역 최솟값에서 멀어질 수 있음\n",
        "    학습률(learning rate)이 너무 작으면 학습시간이 오래걸리고 지역 최솟값에 수렴할 수 있음\n",
        "\n",
        "    1-2-2 배치 경사 하강법(batch Gradient Descent)\n",
        "    배치(batch) : 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양\n",
        "    배치를 전체 데이터로 두는것\n",
        "    1epoch당 시간은 오래 걸리고 메모리를 크게 요구하나, 전역 최솟값을 찾을 수 있다. \n",
        "    model.fit(X_train, y_train, batch_size=len(trainX))\n",
        "\n",
        "    1-2-3 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n",
        "\n",
        "    매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법\n",
        "    더 적은 데이터를 사용하므로 더 빠르게 계산할 수 있다.\n",
        "    때로는 배치 경사 하강법 보다 정확도가 낮을 수 있다.\n",
        "    model.fit(X_train, y_train, batch_size=1)\n",
        "\n",
        "    1-2-4 미니 배치 경사 하강법 (Mini-Batch Gradient Descent)\n",
        "    정해준 데이터 양에 대해서만 계산한여 매개변수 값을 조정한다.\n",
        "    전체 데이터를 계산하는 것보다 빠르며, SGD보다 안정적이다.\n",
        "    실제로 가장 많이 사용되는 경사 하강법이다.\n",
        "    model.fit(X_train, y_train, batch_size=32)  #32를 배치 크기로 하였을 경우\n",
        "\n",
        "    1-2-5 모멘텀(Momentum)\n",
        "    관성이라는 물리학 법칙을 응용한 방법\n",
        "    경사 하강법 + 관성\n",
        "    계산된 접선의 기울기에 한 시점(step)전의 접선의 기울기 값을 일정한 비율만큼 반영\n",
        "    지역 최솟값에 빠지더라도 관성의 힘으로 빠져나올 수 있다.\n",
        "\n",
        "    1-2-6 아담(Adam)\n",
        "    RMSprop와 모멘텀을 합친 방법\n",
        "    방향과 학습률 두 가지를 모두 잡기 위한 방법"
      ],
      "metadata": {
        "id": "1sHJV3k-Snps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), # 배열로 만들어주는 과정\n",
        "  tf.keras.layers.Dense(128, activation='relu'), #차원\n",
        "  tf.keras.layers.Dropout(0.2), #80%의 노드들만 남기고 모두 삭제\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy']) # 훈련에 사용할 옵티마이저(optimizer)와 손실 함수를 선택"
      ],
      "metadata": {
        "id": "gig5M2-KSm1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 훈련 및 평가"
      ],
      "metadata": {
        "id": "gMStWQzcS6Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=5)  #트레이닝셋 모델 훈련, 5번 학습\n",
        "\n",
        "model.evaluate(x_test,  y_test, verbose=2) #테스트셋, 평가 \n",
        "# verbose => 얼마나 값을 정확히 표현할것인가"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBPiSuR4S5si",
        "outputId": "e38cd935-d0fe-4c10-ab93-120381617ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 8s 3ms/step - loss: 0.2984 - accuracy: 0.9136\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1446 - accuracy: 0.9574\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1077 - accuracy: 0.9675\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0882 - accuracy: 0.9728\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0744 - accuracy: 0.9760\n",
            "313/313 - 1s - loss: 0.0760 - accuracy: 0.9748 - 815ms/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07596314698457718, 0.9747999906539917]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}